{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104fb02b",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este proyecto, se aborda el problema de la evaluación de similitud semántica entre pares de oraciones. Este es un desafío fundamental en el procesamiento del lenguaje natural (NLP), con aplicaciones en tareas como recuperación de información, detección de plagio, sistemas de recomendación y más.\n",
    "\n",
    "Este trabajo forma parte de la materia **Deep Learning para NLP**, donde se exploran técnicas avanzadas para resolver problemas complejos en el ámbito del lenguaje natural.\n",
    "\n",
    "Para resolver este problema, se implementan y comparan tres enfoques basados en modelos de lenguaje preentrenados (BERT y variantes optimizadas para similitud semántica). Los modelos utilizados son:\n",
    "\n",
    "1. **BERT + Regresión**: Un modelo basado en BERT con una capa de regresión para predecir la similitud.\n",
    "2. **Siamese BERT**: Un modelo siamés que compara las representaciones de dos oraciones.\n",
    "3. **Cross-Attention Model**: Un modelo que utiliza atención cruzada para capturar interacciones entre las oraciones.\n",
    "\n",
    "El objetivo es entrenar y evaluar estos modelos en el conjunto de datos STS Benchmark, normalizando las etiquetas a un rango de 0 a 1. Finalmente, se validan los modelos con ejemplos de prueba para analizar su desempeño en diferentes niveles de similitud semántica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3239213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de3660",
   "metadata": {},
   "source": [
    "Configuracion para la Reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5c1b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed para asegurar reproducibilidad sin importar el dispositivo en el que se ejecute\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769eb75",
   "metadata": {},
   "source": [
    "### Carga y Preprocesamiento del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "098c8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "dataset = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "\n",
    "# Normalizar las etiquetas (de 0-5 a un rango de 0-1)\n",
    "def normalize_labels(examples):\n",
    "    examples[\"score\"] = [s / 5.0 for s in examples[\"score\"]]\n",
    "    return examples\n",
    "\n",
    "# Aplicar normalización\n",
    "dataset = dataset.map(normalize_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93b18030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['split', 'genre', 'dataset', 'year', 'sid', 'score', 'sentence1', 'sentence2'],\n",
      "        num_rows: 5749\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['split', 'genre', 'dataset', 'year', 'sid', 'score', 'sentence1', 'sentence2'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['split', 'genre', 'dataset', 'year', 'sid', 'score', 'sentence1', 'sentence2'],\n",
      "        num_rows: 1379\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af90c05",
   "metadata": {},
   "source": [
    "Tokenizacion del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e3d9915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b872e3a9b184487e8fab9622e6701f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creamos un tokenizador para convertir las frases en IDs de tokens que el modelo pueda procesar. Usare BERT o un modelo optimizado para similitud semántica\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "#Definiendo la funcion de tokenizacion\n",
    "def tokenize_function(examples):\n",
    "    #Tokenizando sentence1 y sentence2 por separado\n",
    "    #Esto es necesario porque el modelo espera que las entradas sean pares de oraciones\n",
    "    #y no una sola oración.\n",
    "    encodings1 = tokenizer(\n",
    "        examples['sentence1'],\n",
    "        padding =False,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=None # Lo dejo como lista para el dataset\n",
    "    )\n",
    "    encodings2 = tokenizer(\n",
    "        examples['sentence2'],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=None # Lo dejo como lista para el dataset\n",
    "    )\n",
    "    return{\n",
    "        'input_ids1': encodings1['input_ids'],\n",
    "        'attention_mask1': encodings1['attention_mask'],\n",
    "        'input_ids2': encodings2['input_ids'],\n",
    "        'attention_mask2': encodings2['attention_mask'],\n",
    "        'score': examples['score'] # Mantengo la etiqueta original\n",
    "    }\n",
    "    \n",
    "#Tokenizando el dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49367791",
   "metadata": {},
   "source": [
    "Probando 123..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "343660e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 4946, 2003, 2635, 2125, 1012, 102]\n",
      "[101, 2019, 2250, 4946, 2003, 2635, 2125, 1012, 102]\n",
      "8 9\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0][\"input_ids1\"])\n",
    "print(tokenized_datasets[\"train\"][0][\"input_ids2\"])\n",
    "print(len(tokenized_datasets[\"train\"][0][\"input_ids1\"]), len(tokenized_datasets[\"train\"][0][\"input_ids2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844df6c",
   "metadata": {},
   "source": [
    "Creando el Dataset para Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b3a88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo una clase para formatear los datos de entrada correctamente para PyTorch\n",
    "class STSDataset(Dataset):\n",
    "    def __init__(self, data):        \n",
    "        self.input_ids1 = data[\"input_ids1\"]\n",
    "        self.attention_mask1 = data[\"attention_mask1\"]\n",
    "        self.input_ids2 = data[\"input_ids2\"]\n",
    "        self.attention_mask2 = data[\"attention_mask2\"]\n",
    "        self.scores = data[\"score\"]  # Los scores son las etiquetas (labels)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return{\n",
    "            'input_ids1': self.input_ids1[idx],\n",
    "            'attention_mask1': self.attention_mask1[idx],\n",
    "            'input_ids2': self.input_ids2[idx],\n",
    "            'attention_mask2': self.attention_mask2[idx],\n",
    "            'labels': self.scores[idx],\n",
    "        }\n",
    "#Creando el datase de entrenamiento y validacion\n",
    "train_dataset = STSDataset(tokenized_datasets[\"train\"])\n",
    "val_dataset = STSDataset(tokenized_datasets[\"validation\"])\n",
    "test_dataset = STSDataset(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab995de4",
   "metadata": {},
   "source": [
    "# Creando la Funcion para Fine-Tuning en los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b610154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfa010896e44d8e9c458b1ffdd48cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=1e-05, description='learning_rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899ee25fc0924ce386bdbd1a860466f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='freeze_layers:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53990f3b987466c854d9981d974331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=20, description='epochs:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eed921ef0a44e6a8a20656b09980a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='linear', description='scheduler_type:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se crea un Widget para diferentes parametros de configuracion e identificar la mejor opcion.\n",
    "learning_rate = widgets.FloatText(value=1e-5, description=\"learning_rate:\")\n",
    "freeze_layers = widgets.IntText(value=4, description=\"freeze_layers:\")\n",
    "epochs = widgets.IntText(value=20, description=\"epochs:\")\n",
    "scheduler_type = widgets.Text(value='linear', description=\"scheduler_type:\")\n",
    "\n",
    "display(learning_rate, freeze_layers, epochs, scheduler_type)\n",
    "\n",
    "\n",
    "lr = learning_rate.value\n",
    "fl = freeze_layers.value\n",
    "ep = epochs.value\n",
    "st = scheduler_type.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc575a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para congelar capas\n",
    "def freeze_bert_layers(model, num_frozen_layers=fl):\n",
    "    \"\"\"\n",
    "    Congela las primeras `num_frozen_layers` capas del modelo BERT.\n",
    "    \"\"\"\n",
    "    for layer in model.bert.encoder.layer[:num_frozen_layers]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Congelamos las embeddings iniciales también\n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f9d89",
   "metadata": {},
   "source": [
    "### Definiendo los Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d1d49",
   "metadata": {},
   "source": [
    "Modelo 1 : Siamese BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48b902a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo el primer modelo a utilizar, en este caso estaremos utilizando el modelo de Sentence Transformers \"all-MiniLM-L6-v2\"\n",
    "#Creo un modelo basado en BERT con un head de regresión\n",
    "class SentenceSimilarityModelOne(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", freeze_layers=fl):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Congelar capas según parámetro freeze_layers\n",
    "        for i, param in enumerate(self.bert.encoder.layer):\n",
    "            if i < (len(self.bert.encoder.layer) - freeze_layers):\n",
    "                for p in param.parameters():\n",
    "                    p.requires_grad = False\n",
    "                    \n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(384, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        similarity = self.regressor(cls_embeddings)\n",
    "        return similarity.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a63236",
   "metadata": {},
   "source": [
    "Modelo 2: Siamese BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5842d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBERT(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", freeze_layers=fl):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Congelar capas según parámetro freeze_layers\n",
    "        for i, param in enumerate(self.bert.encoder.layer):\n",
    "            if i < (len(self.bert.encoder.layer) - freeze_layers):\n",
    "                for p in param.parameters():\n",
    "                    p.requires_grad = False\n",
    "           \n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(384 * 3, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.bert(input_ids1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]\n",
    "        output2 = self.bert(input_ids2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]\n",
    "\n",
    "        diff = torch.abs(output1 - output2)\n",
    "        mult = output1 * output2\n",
    "        combined = torch.cat([diff, mult, output1], dim=1)\n",
    "\n",
    "        return self.regressor(combined).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2949525",
   "metadata": {},
   "source": [
    "Modelo 3: Cross-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9254d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", freeze_layers=fl):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Congelar capas según parámetro freeze_layers\n",
    "        for i, param in enumerate(self.bert.encoder.layer):\n",
    "            if i < (len(self.bert.encoder.layer) - freeze_layers):\n",
    "                for p in param.parameters():\n",
    "                    p.requires_grad = False\n",
    "           \n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=384, num_heads=8)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(384 * 2, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.bert(input_ids1, attention_mask=attention_mask1).last_hidden_state\n",
    "        output2 = self.bert(input_ids2, attention_mask=attention_mask2).last_hidden_state\n",
    "\n",
    "        attn_output, _ = self.attention(output1, output2, output2)\n",
    "        combined = torch.cat([output1[:, 0, :], attn_output[:, 0, :]], dim=1)\n",
    "\n",
    "        return self.regressor(combined).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a502b",
   "metadata": {},
   "source": [
    "### Configuracion el Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dff63919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding_collator = DataCollatorWithPadding(tokenizer)\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # Separar las entradas para sentence1 y sentence2\n",
    "        features1 = [\n",
    "            {'input_ids': ex['input_ids1'], 'attention_mask': ex['attention_mask1']}\n",
    "            for ex in examples\n",
    "        ]\n",
    "        feactures2 = [\n",
    "            {'input_ids': ex['input_ids2'], 'attention_mask': ex['attention_mask2']}\n",
    "            for ex in examples\n",
    "        ]\n",
    "        labels = [ex['labels'] for ex in examples]\n",
    "        \n",
    "        # Aplicar padding a las entradas de sentence1 y sentence2\n",
    "        batch1 = self.padding_collator(features1)\n",
    "        batch2 = self.padding_collator(feactures2)\n",
    "        \n",
    "        # Combinar las entradas de sentence1 y sentence2 en un solo diccionario\n",
    "        batch = {\n",
    "            'input_ids1': batch1['input_ids'],\n",
    "            'attention_mask1': batch1['attention_mask'],\n",
    "            'input_ids2': batch2['input_ids'],\n",
    "            'attention_mask2': batch2['attention_mask'],\n",
    "            'labels': torch.tensor(labels, dtype=torch.float32)  # Convertir a tensor de float\n",
    "        }\n",
    "        return batch\n",
    "# Creamos el collator personalizado\n",
    "data_collator = CustomDataCollatorWithPadding(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dde5623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Definiendo el optimizador y el scheduler\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,  # Usar el collator personalizado\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator  # Usar el collator personalizado\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator # Uso el collator personalizado\n",
    "\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model1 = SentenceSimilarityModelOne().to(device)\n",
    "model2 = SiameseBERT().to(device)\n",
    "model3 = CrossAttentionModel().to(device)\n",
    "\n",
    "\n",
    "for model in [model1, model2, model3]:\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c94c4e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de input_ids1: torch.Size([8, 26])\n",
      "Shape de input_ids2: torch.Size([8, 27])\n",
      "Ejemplo de input_ids1: tensor([  101,  3956,  4654,  1011,  8645, 19428,  2114,  1005,  6752, 25443,\n",
      "         2278,  1005,  4238,  2162,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "Ejemplo de input_ids2: tensor([ 101, 5611, 4654, 1011, 8645, 5795, 1024, 2053, 3404, 1999, 4105, 2058,\n",
      "        4238, 1516, 2678,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(\"Shape de input_ids1:\", batch[\"input_ids1\"].shape)\n",
    "    print(\"Shape de input_ids2:\", batch[\"input_ids2\"].shape)\n",
    "    print(\"Ejemplo de input_ids1:\", batch[\"input_ids1\"][0])\n",
    "    print(\"Ejemplo de input_ids2:\", batch[\"input_ids2\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249a3e7",
   "metadata": {},
   "source": [
    "Validando el Fine - Tuning de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e682144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT + Regresión:\n",
      "Total Parámetros: 22,812,033, Entrenables: 19,263,105\n",
      "\n",
      "Siamese BERT:\n",
      "Total Parámetros: 23,008,641, Entrenables: 19,459,713\n",
      "\n",
      "Cross-Attention:\n",
      "Total Parámetros: 23,501,697, Entrenables: 19,952,769\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_params(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parámetros: {total_params:,}, Entrenables: {trainable_params:,}\")\n",
    "\n",
    "print(\"\\nBERT + Regresión:\")\n",
    "print_trainable_params(model1)\n",
    "\n",
    "print(\"\\nSiamese BERT:\")\n",
    "print_trainable_params(model2)\n",
    "\n",
    "print(\"\\nCross-Attention:\")\n",
    "print_trainable_params(model3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa98676",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e85a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de evaluación\n",
    "def evaluate_model(model, val_loader, model_type=\"bert\"):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if model_type == \"bert\":\n",
    "                inputs = batch[\"input_ids1\"].to(device)\n",
    "                mask = batch[\"attention_mask1\"].to(device)\n",
    "                lbls = batch[\"labels\"].to(device)\n",
    "                outputs = model(inputs, mask)\n",
    "\n",
    "            elif model_type in [\"siamese\", \"cross\"]:\n",
    "                inputs1 = batch[\"input_ids1\"].to(device)\n",
    "                mask1 = batch[\"attention_mask1\"].to(device)\n",
    "                inputs2 = batch[\"input_ids2\"].to(device)\n",
    "                mask2 = batch[\"attention_mask2\"].to(device)\n",
    "                lbls = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(inputs1, mask1, inputs2, mask2)\n",
    "\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "    correlation, _ = pearsonr(preds, labels)\n",
    "    print(f\"{model_type.upper()} - Pearson Correlation: {correlation:.4f}\")\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c95563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_mse_loss(output, target):\n",
    "#     weights = torch.where(target < 1,2.0,1.0) # para similitudes muy bajas, se penaliza más\n",
    "#     return torch.mean(weights * (output - target) ** 2)\n",
    "\n",
    "# Función para entrenar modelo con diferentes configuraciones\n",
    "def train_model(model, train_loader, val_loader, model_type=\"bert\", epochs=ep, output_dir=\"./checkpoints\"):\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr) #\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    scheduler = get_scheduler(st, optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)\n",
    "    history = []\n",
    "    best_pearson = -float(\"inf\")\n",
    "    best_model_path = os.path.join(output_dir, f\"best_model_{model_type}.pt\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if model_type == \"bert\":\n",
    "                inputs = batch[\"input_ids1\"].to(device)\n",
    "                mask = batch[\"attention_mask1\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                outputs = model(inputs, mask)\n",
    "\n",
    "            elif model_type in [\"siamese\", \"cross\"]:\n",
    "                inputs1 = batch[\"input_ids1\"].to(device)\n",
    "                mask1 = batch[\"attention_mask1\"].to(device)\n",
    "                inputs2 = batch[\"input_ids2\"].to(device)\n",
    "                mask2 = batch[\"attention_mask2\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                outputs = model(inputs1, mask1, inputs2, mask2)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "         # Evaluo después de cada época\n",
    "        pearson_corr = evaluate_model(model, val_loader, model_type)\n",
    "        history.append((avg_train_loss, pearson_corr))\n",
    "        print(f\"{model_type.upper()} - Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Guardar el mejor modelo basado en la correlación de Pearson\n",
    "        if pearson_corr > best_pearson:\n",
    "            best_pearson = pearson_corr\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Mejor modelo guardado en {best_model_path} con correlación de Pearson: {best_pearson:.4f}\")\n",
    "            \n",
    "    # Cargo el mejor modelo despues de entrenar\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"{model_type.upper()} - Entrenamiento finalizado. Mejor modelo cargado: {best_pearson:.4f}\")\n",
    "    \n",
    "    return history, best_pearson # Devuelvo el historial de pérdidas y correlaciones mas el mejor modelo\n",
    "\n",
    "def evaluate_on_test(model, test_loader, model_type=\"bert\"):\n",
    "    person_corr = evaluate_model(model, test_loader, model_type)\n",
    "    print(f\"{model_type.upper()} - Evaluación en test - Pearson Correlation: {person_corr:.4f}\")\n",
    "    return person_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3dc49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_similarity(model, tokenizer, sentence1, sentence2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_input = tokenizer(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        if isinstance(model, SiameseBERT) or isinstance(model, CrossAttentionModel):\n",
    "            input_ids1 = encoded_input[\"input_ids\"]\n",
    "            attention_mask1 = encoded_input[\"attention_mask\"]\n",
    "            input_ids2 = encoded_input[\"input_ids\"]\n",
    "            attention_mask2 = encoded_input[\"attention_mask\"]\n",
    "\n",
    "            similarity = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        else:\n",
    "            similarity = model(encoded_input[\"input_ids\"], encoded_input[\"attention_mask\"])\n",
    "\n",
    "    return similarity.item() * 5  # Volvemos a la escala de 0 a 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7236468",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m batch\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m outputs\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del batch\n",
    "del outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3a56732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando BERT + Regresion...\n",
      "BERT - Pearson Correlation: 0.2137\n",
      "BERT - Epoch 1/20 - Loss: 0.0912\n",
      "Mejor modelo guardado en ./checkpoints\\best_model_bert.pt con correlación de Pearson: 0.2137\n",
      "BERT - Pearson Correlation: 0.2195\n",
      "BERT - Epoch 2/20 - Loss: 0.0778\n",
      "Mejor modelo guardado en ./checkpoints\\best_model_bert.pt con correlación de Pearson: 0.2195\n",
      "BERT - Pearson Correlation: 0.2119\n",
      "BERT - Epoch 3/20 - Loss: 0.0749\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, (model, model_type) \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEntrenando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     history, best_pearson = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     test_results[model_name] = best_pearson\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCorrelación de Pearson en validación para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_pearson\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, model_type, epochs, output_dir)\u001b[39m\n\u001b[32m     39\u001b[39m loss = loss_fn(outputs, labels)\n\u001b[32m     40\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m scheduler.step()\n\u001b[32m     43\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmart\\OneDrive\\Documentos\\GitHub\\deep-learning-to-nlp\\deep-learning-to-nlp\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmart\\OneDrive\\Documentos\\GitHub\\deep-learning-to-nlp\\deep-learning-to-nlp\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmart\\OneDrive\\Documentos\\GitHub\\deep-learning-to-nlp\\deep-learning-to-nlp\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmart\\OneDrive\\Documentos\\GitHub\\deep-learning-to-nlp\\deep-learning-to-nlp\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmart\\OneDrive\\Documentos\\GitHub\\deep-learning-to-nlp\\deep-learning-to-nlp\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmart\\OneDrive\\Documentos\\GitHub\\deep-learning-to-nlp\\deep-learning-to-nlp\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmart\\OneDrive\\Documentos\\GitHub\\deep-learning-to-nlp\\deep-learning-to-nlp\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Entrenando y evaluando los modelos\n",
    "models = {\n",
    "    \"BERT + Regresion\": (model1, \"bert\"),\n",
    "    \"Siamese BERT\": (model2, \"siamese\"),\n",
    "    \"Cross-Attention\": (model3, \"cross\")\n",
    "}\n",
    "\n",
    "test_results = {}\n",
    "for model_name, (model, model_type) in models.items():\n",
    "    print(f\"\\nEntrenando {model_name}...\")\n",
    "    history, best_pearson = train_model(model, train_loader, val_loader, model_type=model_type, epochs=ep)\n",
    "    test_results[model_name] = best_pearson\n",
    "    print(f\"Correlación de Pearson en validación para {model_name}: {best_pearson:.4f}\")\n",
    "    \n",
    "    \n",
    "for model_name, (model, model_type) in models.items():\n",
    "    print(f\"\\nEntrenando {model_name} en el conjunto de test...\")\n",
    "    test_pearson = evaluate_on_test(model, test_loader, model_type)\n",
    "    test_results[model_name] = test_pearson\n",
    "    print(f\"Correlación de Pearson en test para {model_name}: {test_pearson:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4efe4",
   "metadata": {},
   "source": [
    "Validando los 6 ejemplo contra los modelos entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando BERT + Regression:\n",
      "Ejemplo 1: I love eating apples | The capital of France is Paris → Score: 0.31 --> 0\n",
      "Ejemplo 2: I have a black cat | My pet is a dog → Score: -0.41 --> 0\n",
      "Ejemplo 3: He plays soccer on weekends | She enjoys playing tennis on Sundays → Score: -1.03 --> -1\n",
      "Ejemplo 4: The sun is shining in the sky | It is a bright and sunny day → Score: -0.34 --> 0\n",
      "Ejemplo 5: The smartphone has a large screen | This phone features a big display → Score: -0.34 --> 0\n",
      "Ejemplo 6: The dog is barking loudly | The dog is barking loudly → Score: -0.10 --> 0\n",
      "\n",
      "Evaluando Siamese BERT:\n",
      "Ejemplo 1: I love eating apples | The capital of France is Paris → Score: -2.39 --> -2\n",
      "Ejemplo 2: I have a black cat | My pet is a dog → Score: -2.16 --> -2\n",
      "Ejemplo 3: He plays soccer on weekends | She enjoys playing tennis on Sundays → Score: -1.92 --> -2\n",
      "Ejemplo 4: The sun is shining in the sky | It is a bright and sunny day → Score: -1.58 --> -2\n",
      "Ejemplo 5: The smartphone has a large screen | This phone features a big display → Score: -1.68 --> -2\n",
      "Ejemplo 6: The dog is barking loudly | The dog is barking loudly → Score: -1.73 --> -2\n",
      "\n",
      "Evaluando Cross-Attention:\n",
      "Ejemplo 1: I love eating apples | The capital of France is Paris → Score: 0.51 --> 1\n",
      "Ejemplo 2: I have a black cat | My pet is a dog → Score: 0.31 --> 0\n",
      "Ejemplo 3: He plays soccer on weekends | She enjoys playing tennis on Sundays → Score: 0.01 --> 0\n",
      "Ejemplo 4: The sun is shining in the sky | It is a bright and sunny day → Score: 0.40 --> 0\n",
      "Ejemplo 5: The smartphone has a large screen | This phone features a big display → Score: 0.54 --> 1\n",
      "Ejemplo 6: The dog is barking loudly | The dog is barking loudly → Score: 0.47 --> 0\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    (\"I love eating apples\", \"The capital of France is Paris\"),  # Similitud 0 (Completamente distintas)\n",
    "    (\"I have a black cat\", \"My pet is a dog\"),  # Similitud 1 (Diferentes pero relacionadas con mascotas)\n",
    "    (\"He plays soccer on weekends\", \"She enjoys playing tennis on Sundays\"),  # Similitud 2 (Acciones similares, pero no iguales)\n",
    "    (\"The sun is shining in the sky\", \"It is a bright and sunny day\"),  # Similitud 3 (Mismo contexto, expresado diferente)\n",
    "    (\"The smartphone has a large screen\", \"This phone features a big display\"),  # Similitud 4 (Misma idea, palabras diferentes)\n",
    "    (\"The dog is barking loudly\", \"The dog is barking loudly\"),  # Similitud 5 (Frases idénticas)\n",
    "]\n",
    "\n",
    "# Evaluando los modelos en ejemplos de prueba\n",
    "for model_name, (model, model_type) in models.items():\n",
    "    print(f\"\\nEvaluando {model_name} en ejemplos de prueba:\")\n",
    "    for i, (s1, s2) in enumerate(test_sentences):\n",
    "        score = infer_similarity(model, tokenizer, s1, s2)\n",
    "        print(f\"Ejemplo {i+1}: {s1} | {s2} → Score: {score:.2f} --> {round(score)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87d73e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m batch\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#del outputs\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del batch\n",
    "#del outputs\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
