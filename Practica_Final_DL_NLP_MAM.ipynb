{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104fb02b",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este proyecto, se aborda el problema de la evaluación de similitud semántica entre pares de oraciones. Este es un desafío fundamental en el procesamiento del lenguaje natural (NLP), con aplicaciones en tareas como recuperación de información, detección de plagio, sistemas de recomendación y más.\n",
    "\n",
    "Este trabajo forma parte de la materia **Deep Learning para NLP**, donde se exploran técnicas avanzadas para resolver problemas complejos en el ámbito del lenguaje natural.\n",
    "\n",
    "Para resolver este problema, se implementan y comparan tres enfoques basados en modelos de lenguaje preentrenados (BERT y variantes optimizadas para similitud semántica). Los modelos utilizados son:\n",
    "\n",
    "1. **BERT + Regresión**: Un modelo basado en BERT con una capa de regresión para predecir la similitud.\n",
    "2. **Siamese BERT**: Un modelo siamés que compara las representaciones de dos oraciones.\n",
    "3. **Cross-Attention Model**: Un modelo que utiliza atención cruzada para capturar interacciones entre las oraciones.\n",
    "\n",
    "El objetivo es entrenar y evaluar estos modelos en el conjunto de datos STS Benchmark, normalizando las etiquetas a un rango de 0 a 1. Finalmente, se validan los modelos con ejemplos de prueba para analizar su desempeño en diferentes niveles de similitud semántica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de3660",
   "metadata": {},
   "source": [
    "Configuracion para la Reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed para asegurar reproducibilidad sin importar el dispositivo en el que se ejecute\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769eb75",
   "metadata": {},
   "source": [
    "### Carga y Preprocesamiento del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "dataset = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "\n",
    "# Normalizar las etiquetas (de 0-5 a un rango de 0-1)\n",
    "def normalize_labels(examples):\n",
    "    examples[\"score\"] = [s / 5.0 for s in examples[\"score\"]]\n",
    "    return examples\n",
    "\n",
    "# Aplicar normalización\n",
    "dataset = dataset.map(normalize_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af90c05",
   "metadata": {},
   "source": [
    "Tokenizacion del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un tokenizador para convertir las frases en IDs de tokens que el modelo pueda procesar. Usare BERT o un modelo optimizado para similitud semántica\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "#Definiendo la funcion de tokenizacion\n",
    "def tokenize_function(examples):\n",
    "    #Tokenizando sentence1 y sentence2 por separado\n",
    "    #Esto es necesario porque el modelo espera que las entradas sean pares de oraciones\n",
    "    #y no una sola oración.\n",
    "    encodings1 = tokenizer(\n",
    "        examples['sentence1'],\n",
    "        padding =False,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=None # Lo dejo como lista para el dataset\n",
    "    )\n",
    "    encodings2 = tokenizer(\n",
    "        examples['sentence2'],\n",
    "        padding=\"False\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "        return_tensors=None # Lo dejo como lista para el dataset\n",
    "    )\n",
    "    return{\n",
    "        'input_ids1': encodings1['input_ids'],\n",
    "        'attention_mask1': encodings1['attention_mask'],\n",
    "        'input_ids2': encodings2['input_ids'],\n",
    "        'attention_mask2': encodings2['attention_mask'],\n",
    "        'score': examples['score'] # Mantengo la etiqueta original\n",
    "    }\n",
    "    \n",
    "#Tokenizando el dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49367791",
   "metadata": {},
   "source": [
    "Probando 123..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343660e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][0][\"input_ids1\"])\n",
    "print(tokenized_datasets[\"train\"][0][\"input_ids2\"])\n",
    "print(len(tokenized_datasets[\"train\"][0][\"input_ids1\"]), len(tokenized_datasets[\"train\"][0][\"input_ids2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844df6c",
   "metadata": {},
   "source": [
    "Creando el Dataset para Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo una clase para formatear los datos de entrada correctamente para PyTorch\n",
    "class STSDataset(Dataset):\n",
    "    def __init__(self, data):        \n",
    "        self.input_ids = data[\"input_ids\"]\n",
    "        self.attention_mask = data[\"attention_mask\"]\n",
    "        self.scores = data[\"score\"]  # Los scores son las etiquetas (labels)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return{\n",
    "            'input_ids1': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask1': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'input_ids2': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask2': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.scores[idx], dtype=torch.float),\n",
    "        }\n",
    "#Creando el datase de entrenamiento y validacion\n",
    "train_dataset = STSDataset(tokenized_datasets[\"train\"])\n",
    "val_dataset = STSDataset(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab995de4",
   "metadata": {},
   "source": [
    "# Creando la Funcion para Fine-Tuning en los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b610154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8939985d52194e31ba839ae19c3c6999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=1e-05, description='learning_rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7722de3d86d14cf7b0539896477bde7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='freeze_layers:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ef37330c6b4c688247216fb67a393b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=20, description='epochs:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8752d930088b4064b04326171618ca2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='linear', description='scheduler_type:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se crea un Widget para diferentes parametros de configuracion e identificar la mejor opcion.\n",
    "learning_rate = widgets.FloatText(value=1e-5, description=\"learning_rate:\")\n",
    "freeze_layers = widgets.IntText(value=4, description=\"freeze_layers:\")\n",
    "epochs = widgets.IntText(value=20, description=\"epochs:\")\n",
    "scheduler_type = widgets.Text(value='linear', description=\"scheduler_type:\")\n",
    "\n",
    "display(learning_rate, freeze_layers, epochs, scheduler_type)\n",
    "\n",
    "\n",
    "lr = learning_rate.value\n",
    "fl = freeze_layers.value\n",
    "ep = epochs.value\n",
    "st = scheduler_type.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc575a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para congelar capas\n",
    "def freeze_bert_layers(model, num_frozen_layers=fl):\n",
    "    \"\"\"\n",
    "    Congela las primeras `num_frozen_layers` capas del modelo BERT.\n",
    "    \"\"\"\n",
    "    for layer in model.bert.encoder.layer[:num_frozen_layers]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Congelamos las embeddings iniciales también\n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f9d89",
   "metadata": {},
   "source": [
    "### Definiendo los Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d1d49",
   "metadata": {},
   "source": [
    "Modelo 1 : Siamese BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b902a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo el primer modelo a utilizar, en este caso estaremos utilizando el modelo de Sentence Transformers \"all-MiniLM-L6-v2\"\n",
    "#Creo un modelo basado en BERT con un head de regresión\n",
    "class SentenceSimilarityModelOne(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", freeze_layers=fl):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Congelar capas según parámetro freeze_layers\n",
    "        for i, param in enumerate(self.bert.encoder.layer):\n",
    "            if i < (len(self.bert.encoder.layer) - freeze_layers):\n",
    "                for p in param.parameters():\n",
    "                    p.requires_grad = False\n",
    "                    \n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(384, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        similarity = self.regressor(cls_embeddings)\n",
    "        return similarity.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a63236",
   "metadata": {},
   "source": [
    "Modelo 2: Siamese BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBERT(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", freeze_layers=fl):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Congelar capas según parámetro freeze_layers\n",
    "        for i, param in enumerate(self.bert.encoder.layer):\n",
    "            if i < (len(self.bert.encoder.layer) - freeze_layers):\n",
    "                for p in param.parameters():\n",
    "                    p.requires_grad = False\n",
    "           \n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(384 * 3, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.bert(input_ids1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]\n",
    "        output2 = self.bert(input_ids2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]\n",
    "\n",
    "        diff = torch.abs(output1 - output2)\n",
    "        mult = output1 * output2\n",
    "        combined = torch.cat([diff, mult, output1], dim=1)\n",
    "\n",
    "        return self.regressor(combined).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2949525",
   "metadata": {},
   "source": [
    "Modelo 3: Cross-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", freeze_layers=fl):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Congelar capas según parámetro freeze_layers\n",
    "        for i, param in enumerate(self.bert.encoder.layer):\n",
    "            if i < (len(self.bert.encoder.layer) - freeze_layers):\n",
    "                for p in param.parameters():\n",
    "                    p.requires_grad = False\n",
    "           \n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=384, num_heads=8)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(384 * 2, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.bert(input_ids1, attention_mask=attention_mask1).last_hidden_state\n",
    "        output2 = self.bert(input_ids2, attention_mask=attention_mask2).last_hidden_state\n",
    "\n",
    "        attn_output, _ = self.attention(output1, output2, output2)\n",
    "        combined = torch.cat([output1[:, 0, :], attn_output[:, 0, :]], dim=1)\n",
    "\n",
    "        return self.regressor(combined).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a502b",
   "metadata": {},
   "source": [
    "### Configuracion el Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = SentenceSimilarityModelOne().to(device)\n",
    "model2 = SiameseBERT().to(device)\n",
    "model3 = CrossAttentionModel().to(device)\n",
    "\n",
    "for model in [model1, model2, model3]:\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249a3e7",
   "metadata": {},
   "source": [
    "Validando el Fine - Tuning de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e682144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT + Regresión:\n",
      "Total Parámetros: 22,812,033, Entrenables: 19,263,105\n",
      "\n",
      "Siamese BERT:\n",
      "Total Parámetros: 23,008,641, Entrenables: 19,459,713\n",
      "\n",
      "Cross-Attention:\n",
      "Total Parámetros: 23,501,697, Entrenables: 19,952,769\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_params(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parámetros: {total_params:,}, Entrenables: {trainable_params:,}\")\n",
    "\n",
    "print(\"\\nBERT + Regresión:\")\n",
    "print_trainable_params(model1)\n",
    "\n",
    "print(\"\\nSiamese BERT:\")\n",
    "print_trainable_params(model2)\n",
    "\n",
    "print(\"\\nCross-Attention:\")\n",
    "print_trainable_params(model3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa98676",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de evaluación\n",
    "def evaluate_model(model, val_loader, model_type=\"bert\"):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if model_type == \"bert\":\n",
    "                inputs = batch[\"input_ids1\"].to(device)\n",
    "                mask = batch[\"attention_mask1\"].to(device)\n",
    "                lbls = batch[\"labels\"].to(device)\n",
    "                outputs = model(inputs, mask)\n",
    "\n",
    "            elif model_type in [\"siamese\", \"cross\"]:\n",
    "                inputs1 = batch[\"input_ids1\"].to(device)\n",
    "                mask1 = batch[\"attention_mask1\"].to(device)\n",
    "                inputs2 = batch[\"input_ids2\"].to(device)\n",
    "                mask2 = batch[\"attention_mask2\"].to(device)\n",
    "                lbls = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(inputs1, mask1, inputs2, mask2)\n",
    "\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "    correlation, _ = pearsonr(preds, labels)\n",
    "    print(f\"{model_type.upper()} - Pearson Correlation: {correlation:.4f}\")\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(output, target):\n",
    "    weights = torch.where(target < 1,2.0,1.0) # para similitudes muy bajas, se penaliza más\n",
    "    return torch.mean(weights * (output - target) ** 2)\n",
    "\n",
    "# Función para entrenar modelo con diferentes configuraciones\n",
    "def train_model(model, train_loader, val_loader, model_type=\"bert\", epochs=ep):\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr) #\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    scheduler = get_scheduler(st, optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if model_type == \"bert\":\n",
    "                inputs = batch[\"input_ids1\"].to(device)\n",
    "                mask = batch[\"attention_mask1\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                outputs = model(inputs, mask)\n",
    "\n",
    "            elif model_type in [\"siamese\", \"cross\"]:\n",
    "                inputs1 = batch[\"input_ids1\"].to(device)\n",
    "                mask1 = batch[\"attention_mask1\"].to(device)\n",
    "                inputs2 = batch[\"input_ids2\"].to(device)\n",
    "                mask2 = batch[\"attention_mask2\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                outputs = model(inputs1, mask1, inputs2, mask2)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "         # Evaluo después de cada época\n",
    "        pearson_corr = evaluate_model(model, val_loader, model_type)\n",
    "        history.append((avg_train_loss, pearson_corr))\n",
    "        print(f\"{model_type.upper()} - Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    print(f\"{model_type.upper()} - Entrenamiento finalizado.\")\n",
    "    \n",
    "    return history # Devuelvo el historial de pérdidas y correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_similarity(model, tokenizer, sentence1, sentence2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_input = tokenizer(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        if isinstance(model, SiameseBERT) or isinstance(model, CrossAttentionModel):\n",
    "            input_ids1 = encoded_input[\"input_ids\"]\n",
    "            attention_mask1 = encoded_input[\"attention_mask\"]\n",
    "            input_ids2 = encoded_input[\"input_ids\"]\n",
    "            attention_mask2 = encoded_input[\"attention_mask\"]\n",
    "\n",
    "            similarity = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        else:\n",
    "            similarity = model(encoded_input[\"input_ids\"], encoded_input[\"attention_mask\"])\n",
    "\n",
    "    return similarity.item() * 5  # Volvemos a la escala de 0 a 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4efe4",
   "metadata": {},
   "source": [
    "Validando los 6 ejemplo contra los modelos entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando BERT + Regression:\n",
      "Ejemplo 1: I love eating apples | The capital of France is Paris → Score: 0.50 --> 1\n",
      "Ejemplo 2: I have a black cat | My pet is a dog → Score: 0.74 --> 1\n",
      "Ejemplo 3: He plays soccer on weekends | She enjoys playing tennis on Sundays → Score: 0.46 --> 0\n",
      "Ejemplo 4: The sun is shining in the sky | It is a bright and sunny day → Score: 4.03 --> 4\n",
      "Ejemplo 5: The smartphone has a large screen | This phone features a big display → Score: 3.91 --> 4\n",
      "Ejemplo 6: The dog is barking loudly | The dog is barking loudly → Score: 5.05 --> 5\n",
      "\n",
      "Evaluando Siamese BERT:\n",
      "Ejemplo 1: I love eating apples | The capital of France is Paris → Score: 0.68 --> 1\n",
      "Ejemplo 2: I have a black cat | My pet is a dog → Score: 0.97 --> 1\n",
      "Ejemplo 3: He plays soccer on weekends | She enjoys playing tennis on Sundays → Score: 1.19 --> 1\n",
      "Ejemplo 4: The sun is shining in the sky | It is a bright and sunny day → Score: 4.11 --> 4\n",
      "Ejemplo 5: The smartphone has a large screen | This phone features a big display → Score: 4.30 --> 4\n",
      "Ejemplo 6: The dog is barking loudly | The dog is barking loudly → Score: 5.21 --> 5\n",
      "\n",
      "Evaluando Cross-Attention:\n",
      "Ejemplo 1: I love eating apples | The capital of France is Paris → Score: 0.08 --> 0\n",
      "Ejemplo 2: I have a black cat | My pet is a dog → Score: 0.70 --> 1\n",
      "Ejemplo 3: He plays soccer on weekends | She enjoys playing tennis on Sundays → Score: 0.46 --> 0\n",
      "Ejemplo 4: The sun is shining in the sky | It is a bright and sunny day → Score: 4.08 --> 4\n",
      "Ejemplo 5: The smartphone has a large screen | This phone features a big display → Score: 4.01 --> 4\n",
      "Ejemplo 6: The dog is barking loudly | The dog is barking loudly → Score: 5.36 --> 5\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    (\"I love eating apples\", \"The capital of France is Paris\"),  # Similitud 0 (Completamente distintas)\n",
    "    (\"I have a black cat\", \"My pet is a dog\"),  # Similitud 1 (Diferentes pero relacionadas con mascotas)\n",
    "    (\"He plays soccer on weekends\", \"She enjoys playing tennis on Sundays\"),  # Similitud 2 (Acciones similares, pero no iguales)\n",
    "    (\"The sun is shining in the sky\", \"It is a bright and sunny day\"),  # Similitud 3 (Mismo contexto, expresado diferente)\n",
    "    (\"The smartphone has a large screen\", \"This phone features a big display\"),  # Similitud 4 (Misma idea, palabras diferentes)\n",
    "    (\"The dog is barking loudly\", \"The dog is barking loudly\"),  # Similitud 5 (Frases idénticas)\n",
    "]\n",
    "\n",
    "models = {\n",
    "    \"BERT + Regression\": model1,\n",
    "    \"Siamese BERT\": model2,\n",
    "    \"Cross-Attention\": model3\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluando {model_name}:\")\n",
    "    for i, (s1, s2) in enumerate(test_sentences):\n",
    "        score = infer_similarity(model, tokenizer, s1, s2)\n",
    "        print(f\"Ejemplo {i+1}: {s1} | {s2} → Score: {score:.2f} --> {round(score)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
